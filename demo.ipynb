{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary libraries of pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/25 19:40:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"spark://localhost:7077\").appName(\"demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activeSession',\n",
       " '_convert_from_pandas',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_create_dataframe',\n",
       " '_create_from_pandas_with_arrow',\n",
       " '_create_shell_session',\n",
       " '_get_numpy_record_dtype',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedSession',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_jwrapped',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " '_wrapped',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'conf',\n",
       " 'createDataFrame',\n",
       " 'getActiveSession',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'version']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Create Dataframe using different types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True) method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      "    \n",
      "    When ``schema`` is a list of column names, the type of each column\n",
      "    will be inferred from ``data``.\n",
      "    \n",
      "    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "    from ``data``, which should be an RDD of either :class:`Row`,\n",
      "    :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      "    the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      "    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n",
      "    Each record will also be wrapped into a tuple, which can be converted to row later.\n",
      "    \n",
      "    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      "    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 2.1.0\n",
      "       Added verifySchema.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      "        :class:`pandas.DataFrame`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None.  The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      "        ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
      "        We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> l = [('Alice', 1)]\n",
      "    >>> spark.createDataFrame(l).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).collect()\n",
      "    [Row(age=1, name='Alice')]\n",
      "    \n",
      "    >>> rdd = sc.parallelize(l)\n",
      "    >>> spark.createDataFrame(rdd).collect()\n",
      "    [Row(_1='Alice', _2=1)]\n",
      "    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      "    >>> df.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> person = rdd.map(lambda r: Person(*r))\n",
      "    >>> df2 = spark.createDataFrame(person)\n",
      "    >>> df2.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> df3 = spark.createDataFrame(rdd, schema)\n",
      "    >>> df3.collect()\n",
      "    [Row(name='Alice', age=1)]\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      "    [Row(name='Alice', age=1)]\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    [Row(0=1, 1=2)]\n",
      "    \n",
      "    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      "    [Row(a='Alice', b=1)]\n",
      "    >>> rdd = rdd.map(lambda row: row[1])\n",
      "    >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      "    [Row(value=1)]\n",
      "    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    Py4JJavaError: ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of data to prepare data frame\n",
    "person_list = [(\"Berry\",\"\",\"Allen\",1,\"M\"),\n",
    "    (\"Oliver\",\"Queen\",\"\",2,\"M\"),\n",
    "    (\"Robert\",\"\",\"Williams\",3,\"M\"),\n",
    "    (\"Tony\",\"\",\"Stark\",4,\"F\"),\n",
    "    (\"Rajiv\",\"Mary\",\"Kumar\",5,\"F\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "#defining schema for dataset\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", IntegerType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+---+------+\n",
      "|firstname|middlename|lastname| id|gender|\n",
      "+---------+----------+--------+---+------+\n",
      "|    Berry|          |   Allen|  1|     M|\n",
      "|   Oliver|     Queen|        |  2|     M|\n",
      "|   Robert|          |Williams|  3|     M|\n",
      "|     Tony|          |   Stark|  4|     F|\n",
      "|    Rajiv|      Mary|   Kumar|  5|     F|\n",
      "+---------+----------+--------+---+------+\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=person_list, schema=['firstname', 'middlename', 'lastname', 'id', 'gender'])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|Deep|\n",
      "|  2|Shub|\n",
      "+---+----+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [{'id':1, \"name\": 'Deep'},\n",
    "{'id':2, \"name\": 'Shub'}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data = data)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Read CSV file into DataFrame using PySpark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: string (nullable = true)\n",
      " |-- sepal_width: string (nullable = true)\n",
      " |-- petal_length: string (nullable = true)\n",
      " |-- petal_width: string (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('hdfs:/iris.csv', header=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: string (nullable = true)\n",
      " |-- sepal_width: string (nullable = true)\n",
      " |-- petal_length: string (nullable = true)\n",
      " |-- petal_width: string (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option(key='header', value=True).load('hdfs:/iris.csv')\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba7d6a92d9c7295bfdcfceb88786d0d9300abf0a1802f116b1f16ab64cdf39a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
